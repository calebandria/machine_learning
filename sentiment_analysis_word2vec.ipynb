{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fffb6df0",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Word2Vec\n",
    "\n",
    "This notebook implements sentiment analysis on the [Multiclass Sentiment Analysis Dataset](https://huggingface.co/datasets/Sp1786/multiclass-sentiment-analysis-dataset) using three different approaches:\n",
    "\n",
    "1. **Word2Vec (Pre-trained) + Multinomial Logistic Regression**\n",
    "2. **Word2Vec (Pre-trained) + Multilayer Perceptron (MLP)**\n",
    "3. **Standalone Multilayer Perceptron (Neural Network)**\n",
    "\n",
    "## Table of Contents\n",
    "1. Import Libraries & Load Data\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Data Preprocessing\n",
    "4. Pipeline 1: Word2Vec + Logistic Regression\n",
    "5. Pipeline 2: Word2Vec + MLP\n",
    "6. Pipeline 3: Standalone MLP\n",
    "7. Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85b714e",
   "metadata": {},
   "source": [
    "## 1. Import Libraries & Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc3744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install datasets gensim nltk scikit-learn tensorflow matplotlib seaborn wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cfa530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Utilities\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8db6590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from Hugging Face\n",
    "dataset = load_dataset(\"Sp1786/multiclass-sentiment-analysis-dataset\")\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"\\nDataset structure: {dataset}\")\n",
    "print(f\"\\nSplits available: {list(dataset.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102292e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrames for easier manipulation\n",
    "df_train = pd.DataFrame(dataset['train'])\n",
    "df_validation = pd.DataFrame(dataset['validation'])\n",
    "df_test = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(f\"Training set shape: {df_train.shape}\")\n",
    "print(f\"Validation set shape: {df_validation.shape}\")\n",
    "print(f\"Test set shape: {df_test.shape}\")\n",
    "\n",
    "# Combine all data for EDA (we'll split again later)\n",
    "df = pd.concat([df_train, df_validation, df_test], ignore_index=True)\n",
    "print(f\"\\nTotal dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c27204",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f509325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset info\n",
    "print(\"Dataset Overview:\")\n",
    "print(\"=\"*50)\n",
    "df.info()\n",
    "print(\"\\n\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d80a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(\"=\"*50)\n",
    "print(df.isnull().sum())\n",
    "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfda271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print(\"Duplicate Analysis:\")\n",
    "print(\"=\"*50)\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "print(f\"Percentage of duplicates: {(duplicates/len(df))*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a63d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment label distribution\n",
    "print(\"Sentiment Distribution:\")\n",
    "print(\"=\"*50)\n",
    "print(df['sentiment'].value_counts())\n",
    "print(f\"\\nUnique sentiments: {df['sentiment'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f3015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar plot\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c']  # green, blue, red for positive, neutral, negative\n",
    "axes[0].bar(sentiment_counts.index, sentiment_counts.values, color=colors)\n",
    "axes[0].set_title('Sentiment Distribution (Count)', fontsize=14)\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylabel('Count')\n",
    "for i, v in enumerate(sentiment_counts.values):\n",
    "    axes[0].text(i, v + 200, str(v), ha='center', fontsize=11)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%', \n",
    "            colors=colors, explode=(0.02, 0.02, 0.02), startangle=90)\n",
    "axes[1].set_title('Sentiment Distribution (Percentage)', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44b811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(\"Text Statistics:\")\n",
    "print(\"=\"*50)\n",
    "print(df[['text_length', 'word_count']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4095e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize text length distribution by sentiment\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Text length distribution\n",
    "for sentiment in df['sentiment'].unique():\n",
    "    subset = df[df['sentiment'] == sentiment]\n",
    "    axes[0].hist(subset['text_length'], bins=50, alpha=0.6, label=sentiment)\n",
    "axes[0].set_title('Text Length Distribution by Sentiment', fontsize=14)\n",
    "axes[0].set_xlabel('Character Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim(0, 500)  # Focus on majority of data\n",
    "\n",
    "# Word count distribution\n",
    "for sentiment in df['sentiment'].unique():\n",
    "    subset = df[df['sentiment'] == sentiment]\n",
    "    axes[1].hist(subset['word_count'], bins=50, alpha=0.6, label=sentiment)\n",
    "axes[1].set_title('Word Count Distribution by Sentiment', fontsize=14)\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim(0, 100)  # Focus on majority of data\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0073dce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of text statistics by sentiment\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "df.boxplot(column='text_length', by='sentiment', ax=axes[0])\n",
    "axes[0].set_title('Text Length by Sentiment', fontsize=14)\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylabel('Character Count')\n",
    "axes[0].set_ylim(0, 500)\n",
    "\n",
    "df.boxplot(column='word_count', by='sentiment', ax=axes[1])\n",
    "axes[1].set_title('Word Count by Sentiment', fontsize=14)\n",
    "axes[1].set_xlabel('Sentiment')\n",
    "axes[1].set_ylabel('Word Count')\n",
    "axes[1].set_ylim(0, 100)\n",
    "\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04dd2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample texts from each sentiment class\n",
    "print(\"Sample Texts by Sentiment:\")\n",
    "print(\"=\"*80)\n",
    "for sentiment in df['sentiment'].unique():\n",
    "    print(f\"\\n{sentiment.upper()}:\")\n",
    "    print(\"-\"*40)\n",
    "    samples = df[df['sentiment'] == sentiment]['text'].head(3).values\n",
    "    for i, sample in enumerate(samples, 1):\n",
    "        print(f\"{i}. {sample[:150]}...\" if len(sample) > 150 else f\"{i}. {sample}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fcdbef",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8d11fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text for NLP tasks:\n",
    "    - Convert to lowercase\n",
    "    - Remove URLs\n",
    "    - Remove mentions (@username)\n",
    "    - Remove hashtags\n",
    "    - Remove special characters and numbers\n",
    "    - Remove extra whitespaces\n",
    "    - Tokenize\n",
    "    - Remove stopwords\n",
    "    - Lemmatize\n",
    "    \"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    \n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens \n",
    "              if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test preprocessing\n",
    "sample_text = \"I absolutely LOVE this product!!! ðŸ˜ Check it out: https://example.com @user123 #amazing\"\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Preprocessed:\", preprocess_text(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96231ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the entire dataset\n",
    "print(\"Preprocessing texts... This may take a few minutes.\")\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "print(\"Preprocessing completed!\")\n",
    "\n",
    "# Show before and after\n",
    "print(\"\\nBefore and After Preprocessing:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df['text'].iloc[i][:100]}...\")\n",
    "    print(f\"Cleaned:  {df['cleaned_text'].iloc[i][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ea9e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove empty texts after preprocessing\n",
    "initial_size = len(df)\n",
    "df = df[df['cleaned_text'].str.len() > 0]\n",
    "final_size = len(df)\n",
    "print(f\"Removed {initial_size - final_size} empty texts after preprocessing\")\n",
    "print(f\"Final dataset size: {final_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb66be40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "print(\"Label Encoding:\")\n",
    "print(\"=\"*50)\n",
    "for i, label in enumerate(label_encoder.classes_):\n",
    "    print(f\"{label} -> {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe29c4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-validation-test split\n",
    "X = df['cleaned_text'].values\n",
    "y = df['label'].values\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Train size: {len(X_train)}\")\n",
    "print(f\"Validation size: {len(X_val)}\")\n",
    "print(f\"Test size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a68eb7f",
   "metadata": {},
   "source": [
    "## 4. Pipeline 1: Word2Vec + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870842ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Word2Vec model (may take a while on first run)\n",
    "print(\"Loading pre-trained Word2Vec model...\")\n",
    "w2v_model = api.load('word2vec-google-news-300')\n",
    "embedding_dim = w2v_model.vector_size\n",
    "print(f\"Word2Vec model loaded with embedding dimension: {embedding_dim}\")\n",
    "\n",
    "\n",
    "def document_vector(text):\n",
    "    tokens = text.split()\n",
    "    vectors = [w2v_model[word] for word in tokens if word in w2v_model]\n",
    "    if not vectors:\n",
    "        return np.zeros(embedding_dim)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "\n",
    "# Vectorize datasets\n",
    "print(\"Vectorizing datasets with Word2Vec embeddings...\")\n",
    "X_train_w2v = np.vstack([document_vector(text) for text in X_train])\n",
    "X_val_w2v = np.vstack([document_vector(text) for text in X_val])\n",
    "X_test_w2v = np.vstack([document_vector(text) for text in X_test])\n",
    "\n",
    "print(\"Word2Vec embeddings ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf34f23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multinomial logistic regression on Word2Vec embeddings\n",
    "log_reg = LogisticRegression(max_iter=1000, multi_class='multinomial', solver='lbfgs')\n",
    "log_reg.fit(X_train_w2v, y_train)\n",
    "\n",
    "# Evaluate\n",
    "val_preds_lr = log_reg.predict(X_val_w2v)\n",
    "val_proba_lr = log_reg.predict_proba(X_val_w2v)\n",
    "\n",
    "print(\"Validation Accuracy (LogReg + Word2Vec):\", accuracy_score(y_val, val_preds_lr))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, val_preds_lr, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10dbfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for Logistic Regression\n",
    "cm_lr = confusion_matrix(y_val, val_preds_lr)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix: Word2Vec + Logistic Regression')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51963a89",
   "metadata": {},
   "source": [
    "## 5. Pipeline 2: Word2Vec + PyTorch MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3d8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "class Word2VecMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, len(label_encoder.classes_))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    return running_loss / total, correct / total, np.array(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc810e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "train_dataset_w2v = Word2VecDataset(X_train_w2v, y_train)\n",
    "val_dataset_w2v = Word2VecDataset(X_val_w2v, y_val)\n",
    "test_dataset_w2v = Word2VecDataset(X_test_w2v, y_test)\n",
    "\n",
    "train_loader_w2v = DataLoader(train_dataset_w2v, batch_size=64, shuffle=True)\n",
    "val_loader_w2v = DataLoader(val_dataset_w2v, batch_size=128, shuffle=False)\n",
    "test_loader_w2v = DataLoader(test_dataset_w2v, batch_size=128, shuffle=False)\n",
    "\n",
    "model_w2v_mlp = Word2VecMLP(input_dim=embedding_dim, hidden_dim=256, dropout=0.3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_w2v_mlp.parameters(), lr=1e-3)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "best_state = None\n",
    "\n",
    "for epoch in range(15):\n",
    "    train_loss, train_acc = train_model(model_w2v_mlp, train_loader_w2v, criterion, optimizer, device)\n",
    "    val_loss, val_acc, val_preds_w2v = evaluate_model(model_w2v_mlp, val_loader_w2v, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        best_state = model_w2v_mlp.state_dict()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "if best_state is not None:\n",
    "    model_w2v_mlp.load_state_dict(best_state)\n",
    "\n",
    "print(f\"Best validation accuracy: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d998f274",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_mlp, val_acc_mlp, val_preds_mlp = evaluate_model(model_w2v_mlp, val_loader_w2v, criterion, device)\n",
    "print(f\"Validation Accuracy (Word2Vec + PyTorch MLP): {val_acc_mlp:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, val_preds_mlp, target_names=label_encoder.classes_))\n",
    "\n",
    "cm_mlp = confusion_matrix(y_val, val_preds_mlp)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_mlp, annot=True, fmt='d', cmap='Purples', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix: Word2Vec + PyTorch MLP')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9d6858",
   "metadata": {},
   "source": [
    "## 6. Pipeline 3: TF-IDF + PyTorch MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d42276",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(\"TF-IDF matrices prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137740c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseTfidfDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x_dense = self.X[idx].toarray().squeeze()\n",
    "        return torch.tensor(x_dense, dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "class TfidfMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=512, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, len(label_encoder.classes_))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f3dfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_tfidf = SparseTfidfDataset(X_train_tfidf, y_train)\n",
    "val_dataset_tfidf = SparseTfidfDataset(X_val_tfidf, y_val)\n",
    "test_dataset_tfidf = SparseTfidfDataset(X_test_tfidf, y_test)\n",
    "\n",
    "train_loader_tfidf = DataLoader(train_dataset_tfidf, batch_size=64, shuffle=True)\n",
    "val_loader_tfidf = DataLoader(val_dataset_tfidf, batch_size=128, shuffle=False)\n",
    "test_loader_tfidf = DataLoader(test_dataset_tfidf, batch_size=128, shuffle=False)\n",
    "\n",
    "model_tfidf = TfidfMLP(input_dim=X_train_tfidf.shape[1], hidden_dim=512, dropout=0.4).to(device)\n",
    "criterion_tfidf = nn.CrossEntropyLoss()\n",
    "optimizer_tfidf = optim.Adam(model_tfidf.parameters(), lr=1e-3)\n",
    "\n",
    "best_val_acc_tfidf = 0.0\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "best_state_tfidf = None\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_loss, train_acc = train_model(model_tfidf, train_loader_tfidf, criterion_tfidf, optimizer_tfidf, device)\n",
    "    val_loss, val_acc, val_preds_tfidf = evaluate_model(model_tfidf, val_loader_tfidf, criterion_tfidf, device)\n",
    "\n",
    "    print(f\"Epoch {epoch+1:02d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc_tfidf:\n",
    "        best_val_acc_tfidf = val_acc\n",
    "        patience_counter = 0\n",
    "        best_state_tfidf = model_tfidf.state_dict()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "if best_state_tfidf is not None:\n",
    "    model_tfidf.load_state_dict(best_state_tfidf)\n",
    "\n",
    "print(f\"Best validation accuracy: {best_val_acc_tfidf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_tf, val_acc_tf, val_preds_tf = evaluate_model(model_tfidf, val_loader_tfidf, criterion_tfidf, device)\n",
    "print(f\"Validation Accuracy (TF-IDF + PyTorch MLP): {val_acc_tf:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_val, val_preds_tf, target_names=label_encoder.classes_))\n",
    "\n",
    "cm_tf = confusion_matrix(y_val, val_preds_tf)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm_tf, annot=True, fmt='d', cmap='Greens', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix: TF-IDF + PyTorch MLP')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0def08fe",
   "metadata": {},
   "source": [
    "## 7. Test Evaluation & Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f5210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_lr = log_reg.predict(X_test_w2v)\n",
    "test_acc_lr = accuracy_score(y_test, test_preds_lr)\n",
    "\n",
    "_, test_acc_mlp, test_preds_mlp = evaluate_model(model_w2v_mlp, test_loader_w2v, criterion, device)\n",
    "_, test_acc_tf, test_preds_tf = evaluate_model(model_tfidf, test_loader_tfidf, criterion_tfidf, device)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Pipeline': [\n",
    "        'Word2Vec + Logistic Regression',\n",
    "        'Word2Vec + PyTorch MLP',\n",
    "        'TF-IDF + PyTorch MLP'\n",
    "    ],\n",
    "    'Validation Accuracy': [\n",
    "        accuracy_score(y_val, val_preds_lr),\n",
    "        val_acc_mlp,\n",
    "        val_acc_tf\n",
    "    ],\n",
    "    'Test Accuracy': [\n",
    "        test_acc_lr,\n",
    "        test_acc_mlp,\n",
    "        test_acc_tf\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Validation & Test Accuracy Comparison:\")\n",
    "display(comparison_df.sort_values('Test Accuracy', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0279756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test Classification Report - Word2Vec + Logistic Regression:\\n\", classification_report(y_test, test_preds_lr, target_names=label_encoder.classes_))\n",
    "print(\"Test Classification Report - Word2Vec + PyTorch MLP:\\n\", classification_report(y_test, test_preds_mlp, target_names=label_encoder.classes_))\n",
    "print(\"Test Classification Report - TF-IDF + PyTorch MLP:\\n\", classification_report(y_test, test_preds_tf, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a62cabd",
   "metadata": {},
   "source": [
    "## 8. Key Takeaways\n",
    "- Word2Vec features coupled with a PyTorch MLP provide a flexible nonlinear decision boundary.\n",
    "- TF-IDF with a PyTorch MLP can capture nuanced n-gram patterns absent in averaged Word2Vec representations.\n",
    "- Logistic regression remains a competitive baseline and is faster to train, but shows lower recall on minority classes.\n",
    "- Further improvements could leverage contextual embeddings (e.g., transformers) or class-weighted losses to handle any imbalance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
